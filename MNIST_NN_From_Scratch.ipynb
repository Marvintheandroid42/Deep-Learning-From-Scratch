{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm5Qgl5IdveKeZdeKHziS1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marvintheandroid42/Deep-Learning-From-Scratch/blob/main/MNIST_NN_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJfrtZDwdj8W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def import_data(label = 2):\n",
        "\n",
        "  MNIST = np.array(\n",
        "      pd.read_csv('/content/sample_data/mnist_train_small.csv') )\n",
        "\n",
        "  X = MNIST[:, 1:]\n",
        "  y = MNIST[:, 0]\n",
        "\n",
        "\n",
        "  X = (X - np.min(X)) / (np.max(X) - np.min(X)) #scaling the X values\n",
        "\n",
        "  y[np.where(y != label)] = 0\n",
        "  y[np.where(y == label)] = 1 #binary output processing\n",
        "\n",
        "\n",
        "  return X.T, y.reshape(-1,1) #(m, n) (n, 1)"
      ],
      "metadata": {
        "id": "WFPHfBHOqrx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = import_data()"
      ],
      "metadata": {
        "id": "u8Wojx2SquK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense():\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "\n",
        "    #using the random normal sampling to get weights -1 < x < 1\n",
        "\n",
        "    self.W = np.random.randn(input_dim, output_dim)\n",
        "    self.b = np.random.randn(output_dim, 1)\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def forward(self, X):\n",
        "\n",
        "    #making sure the input matrix is of the shape (input_dim, n)\n",
        "\n",
        "    if X.shape[0] != self.input_dim:\n",
        "\n",
        "      X = X.T\n",
        "\n",
        "    self.input = X\n",
        "\n",
        "    # Weighted sum gives an output with shape (output_dim, n)\n",
        "\n",
        "    z = np.dot(self.W.T, X) + self.b\n",
        "\n",
        "    return z\n",
        "\n",
        "  def backward(self, input_grad, learning_rate): #shape of input grad from the activation is (output_dim, n)\n",
        "\n",
        "    weight_grad = (1/input_grad.shape[1]) * np.dot(self.input, input_grad.T) #(input_dim, output_dim)\n",
        "\n",
        "    #need to add the (1/input_grad.shape[1]) as the dot product is the aggregate of all the data points\n",
        "    #so in order to take the mean we need to divide by the number of data points as the sum for the mean\n",
        "    #is already done by the dot product, need to carry the 1/n term from the loss function into the update\n",
        "\n",
        "    bias_grad = np.mean(input_grad, axis=1).reshape(-1,1) #(output_dim, 1)\n",
        "\n",
        "    #we dont need to carry the term for the bias as we are already taking the mean using the numpy function!\n",
        "\n",
        "    output_grad = np.dot(self.W, input_grad) #(input_dim, n)\n",
        "\n",
        "    self.W = self.W - learning_rate * (weight_grad)\n",
        "\n",
        "    self.b = self.b - learning_rate * (bias_grad)\n",
        "\n",
        "    return output_grad\n"
      ],
      "metadata": {
        "id": "5MCntWkThBfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid():\n",
        "\n",
        "  def activation(self, x):\n",
        "\n",
        "    return 1 / (1 + np.exp(-1 * x))\n",
        "\n",
        "  def forward(self, X):\n",
        "\n",
        "    #X is of the shape (output_dim of dense layer, n)\n",
        "\n",
        "    self.input = X\n",
        "\n",
        "    #Output is shape (output_dim, n)\n",
        "\n",
        "    return self.activation(X)\n",
        "\n",
        "  def backward(self, input_grad): #input_grad is of the shape (output_dim, n)\n",
        "\n",
        "    #(output_dim, n) .* (output_dim, n) = (output_dim, n)\n",
        "\n",
        "    output_grad = self.activation(self.input) * self.activation(1 - self.input)\n",
        "\n",
        "    return input_grad * output_grad"
      ],
      "metadata": {
        "id": "yWny21_ikyod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Log_Loss(): #need to compute the forward propogation druing every epoch\n",
        "\n",
        "  def forward(self, y_hat, y): #both arrays should have the shape (1, n)\n",
        "\n",
        "    if y.shape[0] != 1:\n",
        "\n",
        "      y = y.T\n",
        "\n",
        "\n",
        "    if y_hat.shape[0] != 1:\n",
        "\n",
        "      y_hat = y_hat.T\n",
        "\n",
        "\n",
        "    self.y_hat = y_hat\n",
        "\n",
        "    self.y = y\n",
        "\n",
        "    return -1 * np.mean(y * np.log(y_hat) + (1-y)*np.log(1-y_hat), axis=1)\n",
        "\n",
        "\n",
        "  def backward(self):\n",
        "\n",
        "    return -1 * ((self.y / self.y_hat) - ((1-self.y)/(1-self.y_hat))) #(1, n) shape\n"
      ],
      "metadata": {
        "id": "FB_A0Lhpx9dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MSE_Loss():\n",
        "\n",
        "  def forward(self, y_hat, y):\n",
        "\n",
        "    if y.shape[0] != 1:\n",
        "\n",
        "      y = y.T\n",
        "\n",
        "\n",
        "    if y_hat.shape[0] != 1:\n",
        "\n",
        "      y_hat = y_hat.T\n",
        "\n",
        "    self.y_hat = y_hat\n",
        "\n",
        "    self.y = y\n",
        "\n",
        "    return np.mean((y - y_hat)**2, axis=1)\n",
        "\n",
        "  def backward(self):\n",
        "\n",
        "    return 2 * (self.y - self.y_hat)\n",
        "\n"
      ],
      "metadata": {
        "id": "jUeszHSOelKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = Dense(784, 64)\n",
        "l2 = Dense(64, 1)\n",
        "a1 = Sigmoid()\n",
        "a2 = Sigmoid()"
      ],
      "metadata": {
        "id": "0P6hxLSkqfoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(X):\n",
        "  return a2.forward(l2.forward(a1.forward(l1.forward(X))))"
      ],
      "metadata": {
        "id": "u_8NVfV1q_zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(X, y, ALPHA):\n",
        "\n",
        "\n",
        "  loss_func = Log_Loss()\n",
        "\n",
        "\n",
        "  loss = loss_func.forward(forward(X), y)\n",
        "  loss_grad = a2.backward(loss_func.backward()) #(1, n)\n",
        "\n",
        "  l2_grad = a1.backward(l2.backward(loss_grad, ALPHA))\n",
        "\n",
        "  l1_grad = l1.backward(l2_grad, ALPHA)\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "_nXNhhdf91ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(EPOCHS, ALPHA, LAMBDA, verbose=True):\n",
        "\n",
        "  for i in range(EPOCHS): #implement learning rate diminishing using exponential function with respect to the number of epochs\n",
        "\n",
        "  #need to implement dynamic learning rate, regularization and early stopping\n",
        "    loss = backward(X, y, ALPHA*((LAMBDA)**i))\n",
        "\n",
        "    if verbose == True:\n",
        "\n",
        "      print('EPOCH #: ', i, '      ', 'LOSS: ', loss)"
      ],
      "metadata": {
        "id": "-0tefUsKC_HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 15\n",
        "ALPHA = 5 #inital learning rate\n",
        "LAMBDA = 0.7 #diminishing value for learning rate to take smaller steps based on epoch"
      ],
      "metadata": {
        "id": "v_ApdL5UDMxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training(EPOCHS, ALPHA, LAMBDA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcxlqPrxhNNo",
        "outputId": "18245839-a68a-4bb3-ff5f-c8b3458c3c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH #:  0        LOSS:  [0.61514815]\n",
            "EPOCH #:  1        LOSS:  [2.42306346]\n",
            "EPOCH #:  2        LOSS:  [1.44150173]\n",
            "EPOCH #:  3        LOSS:  [0.82935898]\n",
            "EPOCH #:  4        LOSS:  [0.44236343]\n",
            "EPOCH #:  5        LOSS:  [0.2602296]\n",
            "EPOCH #:  6        LOSS:  [0.22739271]\n",
            "EPOCH #:  7        LOSS:  [0.22323973]\n",
            "EPOCH #:  8        LOSS:  [0.22105908]\n",
            "EPOCH #:  9        LOSS:  [0.21961998]\n",
            "EPOCH #:  10        LOSS:  [0.21864021]\n",
            "EPOCH #:  11        LOSS:  [0.21796573]\n",
            "EPOCH #:  12        LOSS:  [0.21749871]\n",
            "EPOCH #:  13        LOSS:  [0.21717418]\n",
            "EPOCH #:  14        LOSS:  [0.21694814]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#UTILS\n",
        "\n",
        "def logits_to_classes(y_hat, threshold):\n",
        "\n",
        "  y_hat[np.where(y_hat >= threshold)] = 1\n",
        "\n",
        "  y_hat[np.where(y_hat < threshold)] = 0\n",
        "\n",
        "  return y_hat\n",
        "\n",
        "def accuracy(y_hat, y, verbose=True):\n",
        "\n",
        "  if y_hat.shape[1] != 1:\n",
        "\n",
        "    y_hat = y_hat.T\n",
        "\n",
        "  if y.shape[1] != 1:\n",
        "\n",
        "    y = y.T\n",
        "\n",
        "\n",
        "  acc = np.round(len(np.where(y_hat == y)[0]) / len(y), decimals=3) * 100\n",
        "\n",
        "  if verbose == True:\n",
        "\n",
        "    print('ACCURACY: ', acc, '%')\n",
        "\n",
        "  return acc\n"
      ],
      "metadata": {
        "id": "xcfxUQkXTsBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = accuracy(logits_to_classes(forward(X), 0.5), y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGVEdplhUXqf",
        "outputId": "f79748f0-f24a-4c72-9001-8dcea64baf59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY:  92.80000000000001 %\n"
          ]
        }
      ]
    }
  ]
}